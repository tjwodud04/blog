---
title: "Fastaiì™€ íŒŒì´í† ì¹˜ê°€ ë§Œë‚˜ ê½ƒí”¼ìš´ ë”¥ëŸ¬ë‹ ì±•í„° 4"
date: 2023-06-15 00:00:00 +0900
categories:
  - fastai
toc: true  
#image : assets/image/how-to-start.jpg # Add image post (optional)
tags:
  - ML
  - MNIST
---
Source :
- [04_mnist_basics.ipynb](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)
- [Fastbook Chapter 4 questionnaire solutions (wiki)](https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253)

### Chapter 4 full code
```
#hide
! [ -e /content ] && pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
```
```
output:  

[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m719.8/719.8 kB[0m [31m49.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m7.2/7.2 MB[0m [31m73.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m485.6/485.6 kB[0m [31m46.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.3/1.3 MB[0m [31m83.0 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m110.5/110.5 kB[0m [31m14.3 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m212.5/212.5 kB[0m [31m28.2 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m134.3/134.3 kB[0m [31m16.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.0/1.0 MB[0m [31m61.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m236.8/236.8 kB[0m [31m29.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m7.8/7.8 MB[0m [31m68.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.3/1.3 MB[0m [31m61.6 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m114.5/114.5 kB[0m [31m5.4 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m268.8/268.8 kB[0m [31m29.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m149.6/149.6 kB[0m [31m16.5 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.6/1.6 MB[0m [31m60.9 MB/s[0m eta [36m0:00:00[0m
[?25hMounted at /content/gdrive
```
```
#hide
from fastai.vision.all import *
from fastbook import *

matplotlib.rc('image', cmap='Greys')
```
```
path = untar_data(URLs.MNIST_SAMPLE)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<div>
  <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.14% [3219456/3214948 00:01&lt;00:00]
</div>

```
#hide
Path.BASE_PATH = path
```
```
path.ls()
```
```
output :  
[Path('valid'),Path('train'),Path('labels.csv')]
```
```
(path/'train').ls()
```
```
output:  
[Path('train/3'),Path('train/7')]
```
```
threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
threes
```
```
output :  
[Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]
```
```
im3_path = threes[1]
im3 = Image.open(im3_path)
im3
```

output :  
![04_mnist_basics_8_0](https://github.com/tjwodud04/blog/assets/34568203/431bee82-eeba-4935-afab-0ab790f0d433)
```
array(im3)[4:10,4:10]
```
```
output :  
array([[  0,   0,   0,   0,   0,   0],  
       [  0,   0,   0,   0,   0,  29],  
       [  0,   0,   0,  48, 166, 224],  
       [  0,  93, 244, 249, 253, 187],  
       [  0, 107, 253, 253, 230,  48],  
       [  0,   3,  20,  20,  15,   0]], dtype=uint8)
```
```
tensor(im3)[4:10,4:10]
```
```
output : 
tensor([[  0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,  29],
        [  0,   0,   0,  48, 166, 224],
        [  0,  93, 244, 249, 253, 187],
        [  0, 107, 253, 253, 230,  48],
        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)
```        
```
#hide_output
im3_t = tensor(im3)
df = pd.DataFrame(im3_t[4:15,4:22])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
```

<style type="text/css">
#T_2120c_row0_col0, #T_2120c_row0_col1, #T_2120c_row0_col2, #T_2120c_row0_col3, #T_2120c_row0_col4, #T_2120c_row0_col5, #T_2120c_row0_col6, #T_2120c_row0_col7, #T_2120c_row0_col8, #T_2120c_row0_col9, #T_2120c_row0_col10, #T_2120c_row0_col11, #T_2120c_row0_col12, #T_2120c_row0_col13, #T_2120c_row0_col14, #T_2120c_row0_col15, #T_2120c_row0_col16, #T_2120c_row0_col17, #T_2120c_row1_col0, #T_2120c_row1_col1, #T_2120c_row1_col2, #T_2120c_row1_col3, #T_2120c_row1_col4, #T_2120c_row1_col15, #T_2120c_row1_col16, #T_2120c_row1_col17, #T_2120c_row2_col0, #T_2120c_row2_col1, #T_2120c_row2_col2, #T_2120c_row2_col15, #T_2120c_row2_col16, #T_2120c_row2_col17, #T_2120c_row3_col0, #T_2120c_row3_col15, #T_2120c_row3_col16, #T_2120c_row3_col17, #T_2120c_row4_col0, #T_2120c_row4_col6, #T_2120c_row4_col7, #T_2120c_row4_col8, #T_2120c_row4_col9, #T_2120c_row4_col10, #T_2120c_row4_col15, #T_2120c_row4_col16, #T_2120c_row4_col17, #T_2120c_row5_col0, #T_2120c_row5_col5, #T_2120c_row5_col6, #T_2120c_row5_col7, #T_2120c_row5_col8, #T_2120c_row5_col9, #T_2120c_row5_col15, #T_2120c_row5_col16, #T_2120c_row5_col17, #T_2120c_row6_col0, #T_2120c_row6_col1, #T_2120c_row6_col2, #T_2120c_row6_col3, #T_2120c_row6_col4, #T_2120c_row6_col5, #T_2120c_row6_col6, #T_2120c_row6_col7, #T_2120c_row6_col8, #T_2120c_row6_col9, #T_2120c_row6_col14, #T_2120c_row6_col15, #T_2120c_row6_col16, #T_2120c_row6_col17, #T_2120c_row7_col0, #T_2120c_row7_col1, #T_2120c_row7_col2, #T_2120c_row7_col3, #T_2120c_row7_col4, #T_2120c_row7_col5, #T_2120c_row7_col6, #T_2120c_row7_col13, #T_2120c_row7_col14, #T_2120c_row7_col15, #T_2120c_row7_col16, #T_2120c_row7_col17, #T_2120c_row8_col0, #T_2120c_row8_col1, #T_2120c_row8_col2, #T_2120c_row8_col3, #T_2120c_row8_col4, #T_2120c_row8_col13, #T_2120c_row8_col14, #T_2120c_row8_col15, #T_2120c_row8_col16, #T_2120c_row8_col17, #T_2120c_row9_col0, #T_2120c_row9_col1, #T_2120c_row9_col2, #T_2120c_row9_col3, #T_2120c_row9_col4, #T_2120c_row9_col16, #T_2120c_row9_col17, #T_2120c_row10_col0, #T_2120c_row10_col1, #T_2120c_row10_col2, #T_2120c_row10_col3, #T_2120c_row10_col4, #T_2120c_row10_col5, #T_2120c_row10_col6, #T_2120c_row10_col17 {
  font-size: 6pt;
  background-color: #ffffff;
  color: #000000;
}
#T_2120c_row1_col5 {
  font-size: 6pt;
  background-color: #efefef;
  color: #000000;
}
#T_2120c_row1_col6, #T_2120c_row1_col13 {
  font-size: 6pt;
  background-color: #7c7c7c;
  color: #f1f1f1;
}
#T_2120c_row1_col7 {
  font-size: 6pt;
  background-color: #4a4a4a;
  color: #f1f1f1;
}
#T_2120c_row1_col8, #T_2120c_row1_col9, #T_2120c_row1_col10, #T_2120c_row2_col5, #T_2120c_row2_col6, #T_2120c_row2_col7, #T_2120c_row2_col11, #T_2120c_row2_col12, #T_2120c_row2_col13, #T_2120c_row3_col4, #T_2120c_row3_col12, #T_2120c_row3_col13, #T_2120c_row4_col1, #T_2120c_row4_col2, #T_2120c_row4_col3, #T_2120c_row4_col12, #T_2120c_row4_col13, #T_2120c_row5_col12, #T_2120c_row6_col11, #T_2120c_row9_col11, #T_2120c_row10_col11, #T_2120c_row10_col12, #T_2120c_row10_col13, #T_2120c_row10_col14, #T_2120c_row10_col15, #T_2120c_row10_col16 {
  font-size: 6pt;
  background-color: #000000;
  color: #f1f1f1;
}
#T_2120c_row1_col11 {
  font-size: 6pt;
  background-color: #606060;
  color: #f1f1f1;
}
#T_2120c_row1_col12 {
  font-size: 6pt;
  background-color: #4d4d4d;
  color: #f1f1f1;
}
#T_2120c_row1_col14 {
  font-size: 6pt;
  background-color: #bbbbbb;
  color: #000000;
}
#T_2120c_row2_col3 {
  font-size: 6pt;
  background-color: #e4e4e4;
  color: #000000;
}
#T_2120c_row2_col4, #T_2120c_row8_col6 {
  font-size: 6pt;
  background-color: #6b6b6b;
  color: #f1f1f1;
}
#T_2120c_row2_col8, #T_2120c_row2_col14, #T_2120c_row3_col14 {
  font-size: 6pt;
  background-color: #171717;
  color: #f1f1f1;
}
#T_2120c_row2_col9, #T_2120c_row3_col11 {
  font-size: 6pt;
  background-color: #4b4b4b;
  color: #f1f1f1;
}
#T_2120c_row2_col10, #T_2120c_row7_col10, #T_2120c_row8_col8, #T_2120c_row8_col10, #T_2120c_row9_col8, #T_2120c_row9_col10 {
  font-size: 6pt;
  background-color: #010101;
  color: #f1f1f1;
}
#T_2120c_row3_col1 {
  font-size: 6pt;
  background-color: #272727;
  color: #f1f1f1;
}
#T_2120c_row3_col2 {
  font-size: 6pt;
  background-color: #0a0a0a;
  color: #f1f1f1;
}
#T_2120c_row3_col3 {
  font-size: 6pt;
  background-color: #050505;
  color: #f1f1f1;
}
#T_2120c_row3_col5 {
  font-size: 6pt;
  background-color: #333333;
  color: #f1f1f1;
}
#T_2120c_row3_col6 {
  font-size: 6pt;
  background-color: #e6e6e6;
  color: #000000;
}
#T_2120c_row3_col7, #T_2120c_row3_col10 {
  font-size: 6pt;
  background-color: #fafafa;
  color: #000000;
}
#T_2120c_row3_col8 {
  font-size: 6pt;
  background-color: #fbfbfb;
  color: #000000;
}
#T_2120c_row3_col9 {
  font-size: 6pt;
  background-color: #fdfdfd;
  color: #000000;
}
#T_2120c_row4_col4 {
  font-size: 6pt;
  background-color: #1b1b1b;
  color: #f1f1f1;
}
#T_2120c_row4_col5 {
  font-size: 6pt;
  background-color: #e0e0e0;
  color: #000000;
}
#T_2120c_row4_col11 {
  font-size: 6pt;
  background-color: #4e4e4e;
  color: #f1f1f1;
}
#T_2120c_row4_col14 {
  font-size: 6pt;
  background-color: #767676;
  color: #f1f1f1;
}
#T_2120c_row5_col1 {
  font-size: 6pt;
  background-color: #fcfcfc;
  color: #000000;
}
#T_2120c_row5_col2, #T_2120c_row5_col3 {
  font-size: 6pt;
  background-color: #f6f6f6;
  color: #000000;
}
#T_2120c_row5_col4, #T_2120c_row7_col7 {
  font-size: 6pt;
  background-color: #f8f8f8;
  color: #000000;
}
#T_2120c_row5_col10, #T_2120c_row10_col7 {
  font-size: 6pt;
  background-color: #e8e8e8;
  color: #000000;
}
#T_2120c_row5_col11 {
  font-size: 6pt;
  background-color: #222222;
  color: #f1f1f1;
}
#T_2120c_row5_col13, #T_2120c_row6_col12 {
  font-size: 6pt;
  background-color: #090909;
  color: #f1f1f1;
}
#T_2120c_row5_col14 {
  font-size: 6pt;
  background-color: #d0d0d0;
  color: #000000;
}
#T_2120c_row6_col10, #T_2120c_row7_col11, #T_2120c_row9_col6 {
  font-size: 6pt;
  background-color: #060606;
  color: #f1f1f1;
}
#T_2120c_row6_col13 {
  font-size: 6pt;
  background-color: #979797;
  color: #f1f1f1;
}
#T_2120c_row7_col8 {
  font-size: 6pt;
  background-color: #b6b6b6;
  color: #000000;
}
#T_2120c_row7_col9 {
  font-size: 6pt;
  background-color: #252525;
  color: #f1f1f1;
}
#T_2120c_row7_col12 {
  font-size: 6pt;
  background-color: #999999;
  color: #f1f1f1;
}
#T_2120c_row8_col5 {
  font-size: 6pt;
  background-color: #f9f9f9;
  color: #000000;
}
#T_2120c_row8_col7 {
  font-size: 6pt;
  background-color: #101010;
  color: #f1f1f1;
}
#T_2120c_row8_col9, #T_2120c_row9_col9 {
  font-size: 6pt;
  background-color: #020202;
  color: #f1f1f1;
}
#T_2120c_row8_col11 {
  font-size: 6pt;
  background-color: #545454;
  color: #f1f1f1;
}
#T_2120c_row8_col12 {
  font-size: 6pt;
  background-color: #f1f1f1;
  color: #000000;
}
#T_2120c_row9_col5 {
  font-size: 6pt;
  background-color: #f7f7f7;
  color: #000000;
}
#T_2120c_row9_col7 {
  font-size: 6pt;
  background-color: #030303;
  color: #f1f1f1;
}
#T_2120c_row9_col12 {
  font-size: 6pt;
  background-color: #181818;
  color: #f1f1f1;
}
#T_2120c_row9_col13 {
  font-size: 6pt;
  background-color: #303030;
  color: #f1f1f1;
}
#T_2120c_row9_col14 {
  font-size: 6pt;
  background-color: #a9a9a9;
  color: #f1f1f1;
}
#T_2120c_row9_col15 {
  font-size: 6pt;
  background-color: #fefefe;
  color: #000000;
}
#T_2120c_row10_col8, #T_2120c_row10_col9 {
  font-size: 6pt;
  background-color: #bababa;
  color: #000000;
}
#T_2120c_row10_col10 {
  font-size: 6pt;
  background-color: #393939;
  color: #f1f1f1;
}
</style>
<table id="T_2120c" class="dataframe">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_2120c_level0_col0" class="col_heading level0 col0" >0</th>
      <th id="T_2120c_level0_col1" class="col_heading level0 col1" >1</th>
      <th id="T_2120c_level0_col2" class="col_heading level0 col2" >2</th>
      <th id="T_2120c_level0_col3" class="col_heading level0 col3" >3</th>
      <th id="T_2120c_level0_col4" class="col_heading level0 col4" >4</th>
      <th id="T_2120c_level0_col5" class="col_heading level0 col5" >5</th>
      <th id="T_2120c_level0_col6" class="col_heading level0 col6" >6</th>
      <th id="T_2120c_level0_col7" class="col_heading level0 col7" >7</th>
      <th id="T_2120c_level0_col8" class="col_heading level0 col8" >8</th>
      <th id="T_2120c_level0_col9" class="col_heading level0 col9" >9</th>
      <th id="T_2120c_level0_col10" class="col_heading level0 col10" >10</th>
      <th id="T_2120c_level0_col11" class="col_heading level0 col11" >11</th>
      <th id="T_2120c_level0_col12" class="col_heading level0 col12" >12</th>
      <th id="T_2120c_level0_col13" class="col_heading level0 col13" >13</th>
      <th id="T_2120c_level0_col14" class="col_heading level0 col14" >14</th>
      <th id="T_2120c_level0_col15" class="col_heading level0 col15" >15</th>
      <th id="T_2120c_level0_col16" class="col_heading level0 col16" >16</th>
      <th id="T_2120c_level0_col17" class="col_heading level0 col17" >17</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_2120c_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_2120c_row0_col0" class="data row0 col0" >0</td>
      <td id="T_2120c_row0_col1" class="data row0 col1" >0</td>
      <td id="T_2120c_row0_col2" class="data row0 col2" >0</td>
      <td id="T_2120c_row0_col3" class="data row0 col3" >0</td>
      <td id="T_2120c_row0_col4" class="data row0 col4" >0</td>
      <td id="T_2120c_row0_col5" class="data row0 col5" >0</td>
      <td id="T_2120c_row0_col6" class="data row0 col6" >0</td>
      <td id="T_2120c_row0_col7" class="data row0 col7" >0</td>
      <td id="T_2120c_row0_col8" class="data row0 col8" >0</td>
      <td id="T_2120c_row0_col9" class="data row0 col9" >0</td>
      <td id="T_2120c_row0_col10" class="data row0 col10" >0</td>
      <td id="T_2120c_row0_col11" class="data row0 col11" >0</td>
      <td id="T_2120c_row0_col12" class="data row0 col12" >0</td>
      <td id="T_2120c_row0_col13" class="data row0 col13" >0</td>
      <td id="T_2120c_row0_col14" class="data row0 col14" >0</td>
      <td id="T_2120c_row0_col15" class="data row0 col15" >0</td>
      <td id="T_2120c_row0_col16" class="data row0 col16" >0</td>
      <td id="T_2120c_row0_col17" class="data row0 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_2120c_row1_col0" class="data row1 col0" >0</td>
      <td id="T_2120c_row1_col1" class="data row1 col1" >0</td>
      <td id="T_2120c_row1_col2" class="data row1 col2" >0</td>
      <td id="T_2120c_row1_col3" class="data row1 col3" >0</td>
      <td id="T_2120c_row1_col4" class="data row1 col4" >0</td>
      <td id="T_2120c_row1_col5" class="data row1 col5" >29</td>
      <td id="T_2120c_row1_col6" class="data row1 col6" >150</td>
      <td id="T_2120c_row1_col7" class="data row1 col7" >195</td>
      <td id="T_2120c_row1_col8" class="data row1 col8" >254</td>
      <td id="T_2120c_row1_col9" class="data row1 col9" >255</td>
      <td id="T_2120c_row1_col10" class="data row1 col10" >254</td>
      <td id="T_2120c_row1_col11" class="data row1 col11" >176</td>
      <td id="T_2120c_row1_col12" class="data row1 col12" >193</td>
      <td id="T_2120c_row1_col13" class="data row1 col13" >150</td>
      <td id="T_2120c_row1_col14" class="data row1 col14" >96</td>
      <td id="T_2120c_row1_col15" class="data row1 col15" >0</td>
      <td id="T_2120c_row1_col16" class="data row1 col16" >0</td>
      <td id="T_2120c_row1_col17" class="data row1 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_2120c_row2_col0" class="data row2 col0" >0</td>
      <td id="T_2120c_row2_col1" class="data row2 col1" >0</td>
      <td id="T_2120c_row2_col2" class="data row2 col2" >0</td>
      <td id="T_2120c_row2_col3" class="data row2 col3" >48</td>
      <td id="T_2120c_row2_col4" class="data row2 col4" >166</td>
      <td id="T_2120c_row2_col5" class="data row2 col5" >224</td>
      <td id="T_2120c_row2_col6" class="data row2 col6" >253</td>
      <td id="T_2120c_row2_col7" class="data row2 col7" >253</td>
      <td id="T_2120c_row2_col8" class="data row2 col8" >234</td>
      <td id="T_2120c_row2_col9" class="data row2 col9" >196</td>
      <td id="T_2120c_row2_col10" class="data row2 col10" >253</td>
      <td id="T_2120c_row2_col11" class="data row2 col11" >253</td>
      <td id="T_2120c_row2_col12" class="data row2 col12" >253</td>
      <td id="T_2120c_row2_col13" class="data row2 col13" >253</td>
      <td id="T_2120c_row2_col14" class="data row2 col14" >233</td>
      <td id="T_2120c_row2_col15" class="data row2 col15" >0</td>
      <td id="T_2120c_row2_col16" class="data row2 col16" >0</td>
      <td id="T_2120c_row2_col17" class="data row2 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_2120c_row3_col0" class="data row3 col0" >0</td>
      <td id="T_2120c_row3_col1" class="data row3 col1" >93</td>
      <td id="T_2120c_row3_col2" class="data row3 col2" >244</td>
      <td id="T_2120c_row3_col3" class="data row3 col3" >249</td>
      <td id="T_2120c_row3_col4" class="data row3 col4" >253</td>
      <td id="T_2120c_row3_col5" class="data row3 col5" >187</td>
      <td id="T_2120c_row3_col6" class="data row3 col6" >46</td>
      <td id="T_2120c_row3_col7" class="data row3 col7" >10</td>
      <td id="T_2120c_row3_col8" class="data row3 col8" >8</td>
      <td id="T_2120c_row3_col9" class="data row3 col9" >4</td>
      <td id="T_2120c_row3_col10" class="data row3 col10" >10</td>
      <td id="T_2120c_row3_col11" class="data row3 col11" >194</td>
      <td id="T_2120c_row3_col12" class="data row3 col12" >253</td>
      <td id="T_2120c_row3_col13" class="data row3 col13" >253</td>
      <td id="T_2120c_row3_col14" class="data row3 col14" >233</td>
      <td id="T_2120c_row3_col15" class="data row3 col15" >0</td>
      <td id="T_2120c_row3_col16" class="data row3 col16" >0</td>
      <td id="T_2120c_row3_col17" class="data row3 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_2120c_row4_col0" class="data row4 col0" >0</td>
      <td id="T_2120c_row4_col1" class="data row4 col1" >107</td>
      <td id="T_2120c_row4_col2" class="data row4 col2" >253</td>
      <td id="T_2120c_row4_col3" class="data row4 col3" >253</td>
      <td id="T_2120c_row4_col4" class="data row4 col4" >230</td>
      <td id="T_2120c_row4_col5" class="data row4 col5" >48</td>
      <td id="T_2120c_row4_col6" class="data row4 col6" >0</td>
      <td id="T_2120c_row4_col7" class="data row4 col7" >0</td>
      <td id="T_2120c_row4_col8" class="data row4 col8" >0</td>
      <td id="T_2120c_row4_col9" class="data row4 col9" >0</td>
      <td id="T_2120c_row4_col10" class="data row4 col10" >0</td>
      <td id="T_2120c_row4_col11" class="data row4 col11" >192</td>
      <td id="T_2120c_row4_col12" class="data row4 col12" >253</td>
      <td id="T_2120c_row4_col13" class="data row4 col13" >253</td>
      <td id="T_2120c_row4_col14" class="data row4 col14" >156</td>
      <td id="T_2120c_row4_col15" class="data row4 col15" >0</td>
      <td id="T_2120c_row4_col16" class="data row4 col16" >0</td>
      <td id="T_2120c_row4_col17" class="data row4 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row5" class="row_heading level0 row5" >5</th>
      <td id="T_2120c_row5_col0" class="data row5 col0" >0</td>
      <td id="T_2120c_row5_col1" class="data row5 col1" >3</td>
      <td id="T_2120c_row5_col2" class="data row5 col2" >20</td>
      <td id="T_2120c_row5_col3" class="data row5 col3" >20</td>
      <td id="T_2120c_row5_col4" class="data row5 col4" >15</td>
      <td id="T_2120c_row5_col5" class="data row5 col5" >0</td>
      <td id="T_2120c_row5_col6" class="data row5 col6" >0</td>
      <td id="T_2120c_row5_col7" class="data row5 col7" >0</td>
      <td id="T_2120c_row5_col8" class="data row5 col8" >0</td>
      <td id="T_2120c_row5_col9" class="data row5 col9" >0</td>
      <td id="T_2120c_row5_col10" class="data row5 col10" >43</td>
      <td id="T_2120c_row5_col11" class="data row5 col11" >224</td>
      <td id="T_2120c_row5_col12" class="data row5 col12" >253</td>
      <td id="T_2120c_row5_col13" class="data row5 col13" >245</td>
      <td id="T_2120c_row5_col14" class="data row5 col14" >74</td>
      <td id="T_2120c_row5_col15" class="data row5 col15" >0</td>
      <td id="T_2120c_row5_col16" class="data row5 col16" >0</td>
      <td id="T_2120c_row5_col17" class="data row5 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row6" class="row_heading level0 row6" >6</th>
      <td id="T_2120c_row6_col0" class="data row6 col0" >0</td>
      <td id="T_2120c_row6_col1" class="data row6 col1" >0</td>
      <td id="T_2120c_row6_col2" class="data row6 col2" >0</td>
      <td id="T_2120c_row6_col3" class="data row6 col3" >0</td>
      <td id="T_2120c_row6_col4" class="data row6 col4" >0</td>
      <td id="T_2120c_row6_col5" class="data row6 col5" >0</td>
      <td id="T_2120c_row6_col6" class="data row6 col6" >0</td>
      <td id="T_2120c_row6_col7" class="data row6 col7" >0</td>
      <td id="T_2120c_row6_col8" class="data row6 col8" >0</td>
      <td id="T_2120c_row6_col9" class="data row6 col9" >0</td>
      <td id="T_2120c_row6_col10" class="data row6 col10" >249</td>
      <td id="T_2120c_row6_col11" class="data row6 col11" >253</td>
      <td id="T_2120c_row6_col12" class="data row6 col12" >245</td>
      <td id="T_2120c_row6_col13" class="data row6 col13" >126</td>
      <td id="T_2120c_row6_col14" class="data row6 col14" >0</td>
      <td id="T_2120c_row6_col15" class="data row6 col15" >0</td>
      <td id="T_2120c_row6_col16" class="data row6 col16" >0</td>
      <td id="T_2120c_row6_col17" class="data row6 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row7" class="row_heading level0 row7" >7</th>
      <td id="T_2120c_row7_col0" class="data row7 col0" >0</td>
      <td id="T_2120c_row7_col1" class="data row7 col1" >0</td>
      <td id="T_2120c_row7_col2" class="data row7 col2" >0</td>
      <td id="T_2120c_row7_col3" class="data row7 col3" >0</td>
      <td id="T_2120c_row7_col4" class="data row7 col4" >0</td>
      <td id="T_2120c_row7_col5" class="data row7 col5" >0</td>
      <td id="T_2120c_row7_col6" class="data row7 col6" >0</td>
      <td id="T_2120c_row7_col7" class="data row7 col7" >14</td>
      <td id="T_2120c_row7_col8" class="data row7 col8" >101</td>
      <td id="T_2120c_row7_col9" class="data row7 col9" >223</td>
      <td id="T_2120c_row7_col10" class="data row7 col10" >253</td>
      <td id="T_2120c_row7_col11" class="data row7 col11" >248</td>
      <td id="T_2120c_row7_col12" class="data row7 col12" >124</td>
      <td id="T_2120c_row7_col13" class="data row7 col13" >0</td>
      <td id="T_2120c_row7_col14" class="data row7 col14" >0</td>
      <td id="T_2120c_row7_col15" class="data row7 col15" >0</td>
      <td id="T_2120c_row7_col16" class="data row7 col16" >0</td>
      <td id="T_2120c_row7_col17" class="data row7 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row8" class="row_heading level0 row8" >8</th>
      <td id="T_2120c_row8_col0" class="data row8 col0" >0</td>
      <td id="T_2120c_row8_col1" class="data row8 col1" >0</td>
      <td id="T_2120c_row8_col2" class="data row8 col2" >0</td>
      <td id="T_2120c_row8_col3" class="data row8 col3" >0</td>
      <td id="T_2120c_row8_col4" class="data row8 col4" >0</td>
      <td id="T_2120c_row8_col5" class="data row8 col5" >11</td>
      <td id="T_2120c_row8_col6" class="data row8 col6" >166</td>
      <td id="T_2120c_row8_col7" class="data row8 col7" >239</td>
      <td id="T_2120c_row8_col8" class="data row8 col8" >253</td>
      <td id="T_2120c_row8_col9" class="data row8 col9" >253</td>
      <td id="T_2120c_row8_col10" class="data row8 col10" >253</td>
      <td id="T_2120c_row8_col11" class="data row8 col11" >187</td>
      <td id="T_2120c_row8_col12" class="data row8 col12" >30</td>
      <td id="T_2120c_row8_col13" class="data row8 col13" >0</td>
      <td id="T_2120c_row8_col14" class="data row8 col14" >0</td>
      <td id="T_2120c_row8_col15" class="data row8 col15" >0</td>
      <td id="T_2120c_row8_col16" class="data row8 col16" >0</td>
      <td id="T_2120c_row8_col17" class="data row8 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row9" class="row_heading level0 row9" >9</th>
      <td id="T_2120c_row9_col0" class="data row9 col0" >0</td>
      <td id="T_2120c_row9_col1" class="data row9 col1" >0</td>
      <td id="T_2120c_row9_col2" class="data row9 col2" >0</td>
      <td id="T_2120c_row9_col3" class="data row9 col3" >0</td>
      <td id="T_2120c_row9_col4" class="data row9 col4" >0</td>
      <td id="T_2120c_row9_col5" class="data row9 col5" >16</td>
      <td id="T_2120c_row9_col6" class="data row9 col6" >248</td>
      <td id="T_2120c_row9_col7" class="data row9 col7" >250</td>
      <td id="T_2120c_row9_col8" class="data row9 col8" >253</td>
      <td id="T_2120c_row9_col9" class="data row9 col9" >253</td>
      <td id="T_2120c_row9_col10" class="data row9 col10" >253</td>
      <td id="T_2120c_row9_col11" class="data row9 col11" >253</td>
      <td id="T_2120c_row9_col12" class="data row9 col12" >232</td>
      <td id="T_2120c_row9_col13" class="data row9 col13" >213</td>
      <td id="T_2120c_row9_col14" class="data row9 col14" >111</td>
      <td id="T_2120c_row9_col15" class="data row9 col15" >2</td>
      <td id="T_2120c_row9_col16" class="data row9 col16" >0</td>
      <td id="T_2120c_row9_col17" class="data row9 col17" >0</td>
    </tr>
    <tr>
      <th id="T_2120c_level0_row10" class="row_heading level0 row10" >10</th>
      <td id="T_2120c_row10_col0" class="data row10 col0" >0</td>
      <td id="T_2120c_row10_col1" class="data row10 col1" >0</td>
      <td id="T_2120c_row10_col2" class="data row10 col2" >0</td>
      <td id="T_2120c_row10_col3" class="data row10 col3" >0</td>
      <td id="T_2120c_row10_col4" class="data row10 col4" >0</td>
      <td id="T_2120c_row10_col5" class="data row10 col5" >0</td>
      <td id="T_2120c_row10_col6" class="data row10 col6" >0</td>
      <td id="T_2120c_row10_col7" class="data row10 col7" >43</td>
      <td id="T_2120c_row10_col8" class="data row10 col8" >98</td>
      <td id="T_2120c_row10_col9" class="data row10 col9" >98</td>
      <td id="T_2120c_row10_col10" class="data row10 col10" >208</td>
      <td id="T_2120c_row10_col11" class="data row10 col11" >253</td>
      <td id="T_2120c_row10_col12" class="data row10 col12" >253</td>
      <td id="T_2120c_row10_col13" class="data row10 col13" >253</td>
      <td id="T_2120c_row10_col14" class="data row10 col14" >253</td>
      <td id="T_2120c_row10_col15" class="data row10 col15" >187</td>
      <td id="T_2120c_row10_col16" class="data row10 col16" >22</td>
      <td id="T_2120c_row10_col17" class="data row10 col17" >0</td>
    </tr>
  </tbody>
</table>

```
seven_tensors = [tensor(Image.open(o)) for o in sevens]
three_tensors = [tensor(Image.open(o)) for o in threes]
len(three_tensors),len(seven_tensors)
```
```
output :  
(6131, 6265)
```
```
show_image(three_tensors[1]);
```
output :  
![04_mnist_basics_13_0](https://github.com/tjwodud04/blog/assets/34568203/6def1657-133c-4580-bd56-a05333c68d57)  
```
stacked_sevens = torch.stack(seven_tensors).float()/255
stacked_threes = torch.stack(three_tensors).float()/255
stacked_threes.shape
```
```
output :  
torch.Size([6131, 28, 28])
```
```
len(stacked_threes.shape)
```
```
output :  
3
```
```
stacked_threes.ndim
```
```
output :  
3
```
```
mean3 = stacked_threes.mean(0)
show_image(mean3);
```
![04_mnist_basics_17_0](https://github.com/tjwodud04/blog/assets/34568203/f1f2165c-f82d-44a9-b88b-33de4ad4f375)
```
mean7 = stacked_sevens.mean(0)
show_image(mean7);
```    
![04_mnist_basics_18_0](https://github.com/tjwodud04/blog/assets/34568203/c00cf046-d511-4732-9566-d59757bda335)
```
a_3 = stacked_threes[1]
show_image(a_3);
```    
![04_mnist_basics_19_0](https://github.com/tjwodud04/blog/assets/34568203/677e436b-c065-46da-b203-1facf559752e)
```
dist_3_abs = (a_3 - mean3).abs().mean()
dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()
dist_3_abs,dist_3_sqr
```
```
output :  
(tensor(0.1114), tensor(0.2021))
```
```
dist_7_abs = (a_3 - mean7).abs().mean()
dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()
dist_7_abs,dist_7_sqr
```
```
output :  
(tensor(0.1586), tensor(0.3021))
```
```
F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()
```
```
output :  
(tensor(0.1586), tensor(0.3021))
```
```
data = [[1,2,3],[4,5,6]]
arr = array (data)
tns = tensor(data)
```
```
arr  # numpy
```
```
output:
array([[1, 2, 3],  
       [4, 5, 6]])
```
```
tns  # pytorch
```
```
output : 
tensor([[1, 2, 3],
       [4, 5, 6]])
```
```
tns[1]
```
```
output :  
tensor([4, 5, 6])
```
```
tns[:,1]
```
```
output :  
tensor([2, 5])
```
```
tns[1,1:3]
```
```
output :  
tensor([5, 6])
```
```
tns+1
```
```
output : 
tensor([[2, 3, 4],
        [5, 6, 7]])
```
```
tns.type()
```
```
output :  
'torch.LongTensor'
```
```
tns*1.5
```
```
output:
tensor([[1.5000, 3.0000, 4.5000],
        [6.0000, 7.5000, 9.0000]]) 
```
```
valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])
valid_3_tens = valid_3_tens.float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])
valid_7_tens = valid_7_tens.float()/255
valid_3_tens.shape,valid_7_tens.shape
```
```
output :  
(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))
```
```
def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))
mnist_distance(a_3, mean3)
```
```
output :  
tensor(0.1114)
```
```
valid_3_dist = mnist_distance(valid_3_tens, mean3)
valid_3_dist, valid_3_dist.shape
```
```
output : 
(tensor([0.1161, 0.1204, 0.1193,  ..., 0.1230, 0.1424, 0.1413]), torch.Size([1010]))
```
```
tensor([1,2,3]) + tensor(1)
```
```
output :  
tensor([2, 3, 4])
```
```
(valid_3_tens-mean3).shape
```
```
output :  
torch.Size([1010, 28, 28])
```
```
def is_3(x): return mnist_distance(x,mean3) < mnist_distance(x,mean7)
```
```
is_3(a_3), is_3(a_3).float()
```
```
(tensor(True), tensor(1.))
```
```
is_3(valid_3_tens)
```
```
output :  
tensor([ True,  True,  True,  ...,  True, False,  True])
```
```
accuracy_3s =      is_3(valid_3_tens).float() .mean()
accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()

accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2
```
```
output :  
(tensor(0.9168), tensor(0.9854), tensor(0.9511))
```
```
#id gradient_descent
#caption The gradient descent process
#alt Graph showing the steps for Gradient Descent
gv('''
init->predict->loss->gradient->step->stop
step->predict[label=repeat]
''')
```
![04_mnist_basics_41_0](https://github.com/tjwodud04/blog/assets/34568203/352cc993-7a77-4e11-9b99-569c168c041e)
```
def f(x): return x**2
```
```
plot_function(f, 'x', 'x**2')
``` 
![04_mnist_basics_43_0](https://github.com/tjwodud04/blog/assets/34568203/a2f0c06c-c6f7-4806-b042-0dc03c2a9d6c)
```
plot_function(f, 'x', 'x**2')
plt.scatter(-1.5, f(-1.5), color='red');
``` 
![04_mnist_basics_44_0](https://github.com/tjwodud04/blog/assets/34568203/42ea168e-8376-4a6d-8c94-a5b13d71ab14)
```
xt = tensor(3.).requires_grad_()
```
```
yt = f(xt)
yt
```
```
output :  
tensor(9., grad_fn=<PowBackward0>)
```
```
yt.backward()
```
```
xt.grad
```
```
output :  
tensor(6.)
```
```
xt = tensor([3.,4.,10.]).requires_grad_()
xt
```
```
output :  
tensor([ 3.,  4., 10.], requires_grad=True)
```
```
def f(x): return (x**2).sum()

yt = f(xt)
yt
```
```
output :  
tensor(125., grad_fn=<SumBackward0>)
```
```
yt.backward()
xt.grad
```
```
output :  
tensor([ 6.,  8., 20.])
```
```
time = torch.arange(0,20).float(); time
```
```
output :  
tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])
```
```
speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1
plt.scatter(time,speed);
```
![04_mnist_basics_53_0](https://github.com/tjwodud04/blog/assets/34568203/4a0fe394-c019-4bb8-acd0-02162e5558f7)
```
def f(t, params):
    a,b,c = params
    return a*(t**2) + (b*t) + c
```
```
def mse(preds, targets): return ((preds-targets)**2).mean()
```
```
params = torch.randn(3).requires_grad_()
```
```
#hide
orig_params = params.clone()
```
```
preds = f(time, params)
```
```
def show_preds(preds, ax=None):
    if ax is None: ax=plt.subplots()[1]
    ax.scatter(time, speed)
    ax.scatter(time, to_np(preds), color='red')
    ax.set_ylim(-300,100)
```
```
show_preds(preds)
``` 
![04_mnist_basics_60_0](https://github.com/tjwodud04/blog/assets/34568203/b66dd7a1-ad1a-42b8-8051-e67819591822)
```
loss = mse(preds, speed)
loss
```
```
output :  
tensor(25823.8086, grad_fn=<MeanBackward0>)
```
```
loss.backward()
params.grad
```
```
output :  
tensor([-53195.8633,  -3419.7148,   -253.8908])
```
```
params.grad * 1e-5
```
```
output :  
tensor([-0.5320, -0.0342, -0.0025])
```
```
params
```
```
output :  
tensor([-0.7658, -0.7506,  1.3525], requires_grad=True)
```
```
lr = 1e-5
params.data -= lr * params.grad.data
params.grad = None
```
```
preds = f(time,params)
mse(preds, speed)
```
```
output :  
tensor(5435.5356, grad_fn=<MeanBackward0>)
```
```
show_preds(preds)
```
![04_mnist_basics_67_0](https://github.com/tjwodud04/blog/assets/34568203/2d0b5200-7d34-4ba0-a92d-d3298749ede7)
```
def apply_step(params, prn=True):
    preds = f(time, params)
    loss = mse(preds, speed)
    loss.backward()
    params.data -= lr * params.grad.data
    params.grad = None
    if prn: print(loss.item())
    return preds
```
```
for i in range(10): apply_step(params)
```
```
output :  
5435.53564453125  
1577.44921875  
847.3778076171875  
709.2225341796875  
683.0758056640625  
678.1243896484375  
677.1838989257812  
677.0023803710938  
676.9645385742188  
676.9537353515625  
```    
```
#hide
params = orig_params.detach().requires_grad_()
```
```
_,axs = plt.subplots(1,4,figsize=(12,3))
for ax in axs: show_preds(apply_step(params, False), ax)
plt.tight_layout()
``` 
![04_mnist_basics_71_0](https://github.com/tjwodud04/blog/assets/34568203/fe882884-3548-4341-af84-6c3d5713d392)
```
#hide_input
#id gradient_descent
#caption The gradient descent process
#alt Graph showing the steps for Gradient Descent
gv('''
init->predict->loss->gradient->step->stop
step->predict[label=repeat]
''')
```    
![04_mnist_basics_72_0](https://github.com/tjwodud04/blog/assets/34568203/f382d8f4-a6c1-41f9-874a-c76f25aaa4dc)
```
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)
```
```
train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
train_x.shape,train_y.shape
```
```
output :  
(torch.Size([12396, 784]), torch.Size([12396, 1]))
```
```
dset = list(zip(train_x,train_y))
x,y = dset[0]
x.shape,y
```
```
output :  
(torch.Size([784]), tensor([1]))
```
```
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)
valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))
```
```
def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
```
```
weights = init_params((28*28,1))
```
```
bias = init_params(1)
```
```
(train_x[0]*weights.T).sum() + bias
```
```
output :  
tensor([20.2336], grad_fn=<AddBackward0>)`
```
```
def linear1(xb): return xb@weights + bias
preds = linear1(train_x)
preds
```
```
output : 
tensor([[20.2336],
        [17.0644],
        [15.2384],
        ...,
        [18.3804],
        [23.8567],
        [28.6816]], grad_fn=<AddBackward0>)
```
```
corrects = (preds>0.0).float() == train_y
corrects
```
```
output : 
tensor([[ True],
        [ True],
        [ True],
        ...,
        [False],
        [False],
        [False]])
```
```
corrects.float().mean().item()
```
```
output :  
0.4912068545818329
```
```
with torch.no_grad(): weights[0] *= 1.0001
```
```
preds = linear1(train_x)
((preds>0.0).float() == train_y).float().mean().item()
```
```
output :  
0.4912068545818329
```
```
trgts  = tensor([1,0,1])
prds   = tensor([0.9, 0.4, 0.2])
```
```
def mnist_loss(predictions, targets):
    return torch.where(targets==1, 1-predictions, predictions).mean()
```
```
torch.where(trgts==1, 1-prds, prds)
```
```
output : 
tensor([0.1000, 0.4000, 0.8000])
```
```
mnist_loss(prds,trgts)
```
```
output :  
tensor(0.4333)
```
```
mnist_loss(tensor([0.9, 0.4, 0.8]),trgts)
```
```
output :  
tensor(0.2333)
```
```
def sigmoid(x): return 1/(1+torch.exp(-x))
```
```
plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)
``` 
![04_mnist_basics_92_0](https://github.com/tjwodud04/blog/assets/34568203/164d604e-48d1-4158-8c26-4982a1509b3d)
```
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
```
```
coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
list(dl)
```
```
output : 
[tensor([ 3, 12,  8, 10,  2]),
 tensor([ 9,  4,  7, 14,  5]),
 tensor([ 1, 13,  0,  6, 11])]
```
```
ds = L(enumerate(string.ascii_lowercase))
ds
```
```
output :  
[(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]
```
```
dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)
```
```
output : 
[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
 (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),
 (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),
 (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),
 (tensor([2, 4]), ('c', 'e'))]
```
```
weights = init_params((28*28,1))
bias = init_params(1)
```
```
dl = DataLoader(dset, batch_size=256)
xb,yb = first(dl)
xb.shape,yb.shape
```
```
output : 
(torch.Size([256, 784]), torch.Size([256, 1]))
```
```
valid_dl = DataLoader(valid_dset, batch_size=256)
```
```
batch = train_x[:4]
batch.shape
```
```
output :  
torch.Size([4, 784])
```
```
preds = linear1(batch)
preds
```
```
output : 
tensor([[-2.1876],
        [-8.3973],
        [ 2.5000],
        [-4.9473]], grad_fn=<AddBackward0>)
```
```
loss = mnist_loss(preds, train_y[:4])
loss
```
```
output :  
tensor(0.7419, grad_fn=<MeanBackward0>)
```
```
loss.backward()
weights.grad.shape,weights.grad.mean(),bias.grad
```
```
output :  
(torch.Size([784, 1]), tensor(-0.0061), tensor([-0.0420]))
```
```
def calc_grad(xb, yb, model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
```
```
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
```
```
output :  
(tensor(-0.0121), tensor([-0.0840]))
```
```
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
```
```
output :  
(tensor(-0.0182), tensor([-0.1260]))
```
```
weights.grad.zero_()
bias.grad.zero_();
```
```
def train_epoch(model, lr, params):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
```
```
(preds>0.0).float() == train_y[:4]
```
```
output : 
tensor([[False],
        [False],
        [ True],
        [False]])
```
```
def batch_accuracy(xb, yb):
    preds = xb.sigmoid()
    correct = (preds>0.5) == yb
    return correct.float().mean()
```
```
batch_accuracy(linear1(batch), train_y[:4])
```
```
output :  
tensor(0.2500)
```
```
def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
    return round(torch.stack(accs).mean().item(), 4)
```
```
validate_epoch(linear1)
```
```
output :  
0.5263
```
```
lr = 1.
params = weights,bias
train_epoch(linear1, lr, params)
validate_epoch(linear1)
```
```
output :  
0.6663
```
```
for i in range(20):
    train_epoch(linear1, lr, params)
    print(validate_epoch(linear1), end=' ')
```
```
output :  
0.8265 0.89 0.9183 0.9276 0.9398 0.9467 0.9506 0.9525 0.956 0.9579 0.9599 0.9608 0.9613 0.9618 0.9633 0.9638 0.9647 0.9657 0.9672 0.9677
```
```
linear_model = nn.Linear(28*28,1)
```
```
w,b = linear_model.parameters()
w.shape,b.shape
```
```
output :  
(torch.Size([1, 784]), torch.Size([1]))
```
```
class BasicOptim:
    def __init__(self,params,lr): self.params,self.lr = list(params),lr

    def step(self, *args, **kwargs):
        for p in self.params: p.data -= p.grad.data * self.lr

    def zero_grad(self, *args, **kwargs):
        for p in self.params: p.grad = None
```
```
opt = BasicOptim(linear_model.parameters(), lr)
```
```
def train_epoch(model):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        opt.step()
        opt.zero_grad()
```
```
validate_epoch(linear_model)
```
```
output :  
0.4607
```
```
def train_model(model, epochs):
    for i in range(epochs):
        train_epoch(model)
        print(validate_epoch(model), end=' ')
```
```
train_model(linear_model, 20)
```
```
output :  
0.4932 0.7685 0.8554 0.9135 0.9345 0.9482 0.957 0.9633 0.9658 0.9677 0.9697 0.9716 0.9736 0.9745 0.976 0.977 0.9775 0.9775 0.978 0.9785
```
```
linear_model = nn.Linear(28*28,1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)
```
```
output :  
0.4932 0.8179 0.8496 0.914 0.9345 0.9482 0.957 0.9619 0.9658 0.9672 0.9692 0.9712 0.9741 0.9751 0.976 0.9775 0.9775 0.978 0.9785 0.979
```
```
dls = DataLoaders(dl, valid_dl)
```
```
learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,
                loss_func=mnist_loss, metrics=batch_accuracy)
```
```
learn.fit(10, lr=lr)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.636709</td>
      <td>0.503144</td>
      <td>0.495584</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.429828</td>
      <td>0.248517</td>
      <td>0.777233</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.161680</td>
      <td>0.155361</td>
      <td>0.861629</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.072948</td>
      <td>0.097722</td>
      <td>0.917566</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.040128</td>
      <td>0.073205</td>
      <td>0.936212</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.027210</td>
      <td>0.059466</td>
      <td>0.950442</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.021837</td>
      <td>0.050799</td>
      <td>0.957802</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.019398</td>
      <td>0.044980</td>
      <td>0.964181</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.018122</td>
      <td>0.040853</td>
      <td>0.966143</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.017330</td>
      <td>0.037788</td>
      <td>0.968106</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>

```
def simple_net(xb):
    res = xb@w1 + b1
    res = res.max(tensor(0.0))
    res = res@w2 + b2
    return res
```
```
w1 = init_params((28*28,30))
b1 = init_params(30)
w2 = init_params((30,1))
b2 = init_params(1)
```
```
plot_function(F.relu)
```    
![04_mnist_basics_130_0](https://github.com/tjwodud04/blog/assets/34568203/bcddaf9c-a734-4c6c-8b22-aab88f66bfa8)
```
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)
```
```
learn = Learner(dls, simple_net, opt_func=SGD,
                loss_func=mnist_loss, metrics=batch_accuracy)
```
```
#hide_output
learn.fit(40, 0.1)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.333021</td>
      <td>0.396112</td>
      <td>0.512267</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.152461</td>
      <td>0.235238</td>
      <td>0.797350</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.083573</td>
      <td>0.117471</td>
      <td>0.911678</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.054309</td>
      <td>0.078720</td>
      <td>0.940628</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.040829</td>
      <td>0.061228</td>
      <td>0.956330</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.034006</td>
      <td>0.051490</td>
      <td>0.963690</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.030123</td>
      <td>0.045381</td>
      <td>0.966634</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.027619</td>
      <td>0.041218</td>
      <td>0.968106</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.025825</td>
      <td>0.038200</td>
      <td>0.969087</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.024441</td>
      <td>0.035901</td>
      <td>0.969578</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.023321</td>
      <td>0.034082</td>
      <td>0.971541</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.022387</td>
      <td>0.032598</td>
      <td>0.972031</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.021592</td>
      <td>0.031353</td>
      <td>0.974485</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.020904</td>
      <td>0.030284</td>
      <td>0.975466</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.020300</td>
      <td>0.029352</td>
      <td>0.975466</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.019766</td>
      <td>0.028526</td>
      <td>0.975466</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.019288</td>
      <td>0.027788</td>
      <td>0.976448</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.018857</td>
      <td>0.027124</td>
      <td>0.977429</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.018465</td>
      <td>0.026523</td>
      <td>0.978410</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.018107</td>
      <td>0.025977</td>
      <td>0.978901</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.017777</td>
      <td>0.025479</td>
      <td>0.978901</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.017473</td>
      <td>0.025022</td>
      <td>0.979392</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.017191</td>
      <td>0.024601</td>
      <td>0.980373</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.016927</td>
      <td>0.024214</td>
      <td>0.980373</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.016680</td>
      <td>0.023855</td>
      <td>0.981354</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.016449</td>
      <td>0.023521</td>
      <td>0.981354</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.016230</td>
      <td>0.023211</td>
      <td>0.981354</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.016023</td>
      <td>0.022922</td>
      <td>0.981354</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.015827</td>
      <td>0.022653</td>
      <td>0.981845</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.015641</td>
      <td>0.022401</td>
      <td>0.981845</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.015463</td>
      <td>0.022165</td>
      <td>0.981845</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.015294</td>
      <td>0.021944</td>
      <td>0.983317</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.015132</td>
      <td>0.021736</td>
      <td>0.982826</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.014977</td>
      <td>0.021541</td>
      <td>0.982826</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.014828</td>
      <td>0.021357</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.014686</td>
      <td>0.021184</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.014549</td>
      <td>0.021019</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.014417</td>
      <td>0.020864</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.014290</td>
      <td>0.020716</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.014168</td>
      <td>0.020576</td>
      <td>0.982336</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>

```
plt.plot(L(learn.recorder.values).itemgot(2));
```    
![04_mnist_basics_134_0](https://github.com/tjwodud04/blog/assets/34568203/495cb618-c4a6-4f9f-bc26-93e679c3c119)
```
learn.recorder.values[-1][2]
```
```
output :  
0.98233562707901
```
```
dls = ImageDataLoaders.from_folder(path)
learn = vision_learner(dls, resnet18, pretrained=False,
                    loss_func=F.cross_entropy, metrics=accuracy)
learn.fit_one_cycle(1, 0.1)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.069452</td>
      <td>0.016400</td>
      <td>0.998037</td>
      <td>00:24</td>
    </tr>
  </tbody>
</table>

### Questionnaire

1. How is a grayscale image represented on a computer? How about a color image?  
ì´ë¯¸ì§€ëŠ” ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” í”½ì…€ ê°’ì´ ìˆëŠ” ë°°ì—´ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ì˜ ê²½ìš°, ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” í”½ì…€ì— 256ê°œì˜ ì •ìˆ˜ ë²”ìœ„ë¡œ êµ¬ì„±ëœ 2ì°¨ì› ë°°ì—´ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. 0 ê°’ì€ í°ìƒ‰, 255 ê°’ì€ ê²€ì€ìƒ‰, ê·¸ ì‚¬ì´ì—ëŠ” ë‹¤ì–‘í•œ íšŒìƒ‰ì¡° ìŒì˜ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì»¬ëŸ¬ ì´ë¯¸ì§€ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ 3ê°œì˜ ì»¬ëŸ¬ ì±„ë„(ë¹¨ê°•, ë…¹ìƒ‰, íŒŒë‘)ì´ ì‚¬ìš©ë˜ë©°, ê° ì±„ë„ë§ˆë‹¤ ë³„ë„ì˜ 256 ë²”ìœ„ 2D ë°°ì—´ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. í”½ì…€ ê°’ 0ì€ ë‹¤ì‹œ í°ìƒ‰ì„ ë‚˜íƒ€ë‚´ë©° 255ëŠ” ë‹¨ìƒ‰ ë¹¨ê°•, ë…¹ìƒ‰ ë˜ëŠ” íŒŒë‘ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 3ê°œì˜ 2D ë°°ì—´ì€ ì»¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìµœì¢… 3D ë°°ì—´(ë­í¬ 3 í…ì„œ)ì„ í˜•ì„±í•©ë‹ˆë‹¤.
  

2. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?  
trainê³¼ validationì´ë¼ëŠ” ë‘ ê°œì˜ í•˜ìœ„ í´ë”ê°€ ìˆìœ¼ë©°, ì „ìëŠ” ëª¨ë¸ í›ˆë ¨ìš© ë°ì´í„°ë¥¼ í¬í•¨í•˜ê³  í›„ìëŠ” ê° í›ˆë ¨ ë‹¨ê³„ í›„ì— ëª¨ë¸ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ìœ íš¨ì„± ê²€ì‚¬ ì§‘í•©ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë‘ ê°€ì§€ ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
a) ì •í™•ë„ì™€ ê°™ì´ ì‚¬ëŒì´ í•´ì„í•  ìˆ˜ ìˆëŠ” ë©”íŠ¸ë¦­ì„ ë³´ê³ í•˜ê³ (ì¢…ì¢… í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ì¶”ìƒì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë‹¬ë¦¬),  
b) í›ˆë ¨ë˜ì§€ ì•Šì€ ë°ì´í„° ì§‘í•©ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ê³¼ì í•©ì„ ì‰½ê²Œ ê°ì§€í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤(ì¦‰, ê³¼ì í•© ëª¨ë¸ì€ train setì—ì„œëŠ” ì ì  ë” ì˜ ìˆ˜í–‰ë˜ì§€ë§Œ validation setì—ì„œëŠ” ì ì  ë” ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.).  
ë¬¼ë¡  ëª¨ë“  ì‹¤ë¬´ìê°€ ë°ì´í„°ì˜ train/validation ë¶„í• ì„ ì§ì ‘ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê³µê°œ ë°ì´í„° ì„¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ implementation/publication ê°„ì˜ ê²°ê³¼ ë¹„êµë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ ë¶„í• ë©ë‹ˆë‹¤.
ê° í•˜ìœ„ í´ë”ì—ëŠ” ê° ì´ë¯¸ì§€ í´ë˜ìŠ¤ì— ëŒ€í•œ .jpg íŒŒì¼ì´ í¬í•¨ëœ ë‘ ê°œì˜ í•˜ìœ„ í´ë” 3ê³¼ 7ì´ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì‚¬ì§„ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì „ì²´ MNIST ë°ì´í„° ì„¸íŠ¸ì˜ ê²½ìš° ê° ìˆ«ìì— ëŒ€í•œ ì´ë¯¸ì§€ì— ëŒ€í•´ í•˜ë‚˜ì”© ì´ 10ê°œì˜ í•˜ìœ„ í´ë”ê°€ ìˆìŠµë‹ˆë‹¤.
  

3. Explain how the "pixel similarity" approach to classifying digits works.
"í”½ì…€ ìœ ì‚¬ì„±" ì ‘ê·¼ ë°©ì‹ì—ì„œëŠ” ì‹ë³„í•˜ë ¤ëŠ” ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì›í˜•ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê²½ìš° 3ì˜ ì´ë¯¸ì§€ì™€ 7ì˜ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•˜ê³ ì í•©ë‹ˆë‹¤. ì „í˜•ì ì¸ 3ì„ train setì— ìˆëŠ” ëª¨ë“  3ì˜ í”½ì…€ ë‹¨ìœ„ í‰ê· ê°’ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. 7ì˜ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë‘ ê°€ì§€ ì›í˜•ì„ ì‹œê°í™”í•˜ë©´ ì‹¤ì œë¡œëŠ” ìˆ«ìê°€ íë¦¿í•˜ê²Œ í‘œí˜„ëœ ë²„ì „ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì „ì— ë³´ì´ì§€ ì•Šë˜ ì´ë¯¸ì§€ê°€ 3ì¸ì§€ 7ì¸ì§€ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ë‘ ì›í˜•ê³¼ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (ì—¬ê¸°ì„œëŠ” í‰ê·  í”½ì…€ ë‹¨ìœ„ì˜ ì ˆëŒ€ì  ì°¨ì´). ìƒˆë¡œìš´ ì´ë¯¸ì§€ì˜ ì›í˜• 3ê³¼ì˜ ê±°ë¦¬ê°€ ì›í˜• 7ì˜ ë‘ ê°œë³´ë‹¤ ì‘ìœ¼ë©´ 3ì´ë¼ê³  ë§í•©ë‹ˆë‹¤.
  

4. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.
ë¦¬ìŠ¤íŠ¸(ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ë°°ì—´)ëŠ” ì¢…ì¢… for-ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë©ë‹ˆë‹¤. list comprehensionì€ for-ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª©ë¡ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ë‹¨ì¼ í‘œí˜„ì‹ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” íŒŒì´ì¬ ë°©ì‹ì…ë‹ˆë‹¤. ì¢…ì¢… í•„í„°ë§ì„ ìœ„í•œ if ì ˆë„ í¬í•¨ë©ë‹ˆë‹¤.  
ex)
```python
lst_in = range(10)
lst_out = [2*el for el in lst_in if el%2==1] #list comprehension
# is equivalent to:
lst_out = []
for el in lst_in:
       if el%2==1:
       lst_out.append(2*el)
```

5. What is a "rank-3 tensor"?  
í…ì„œì˜ ìˆœìœ„ëŠ” í…ì„œê°€ ê°€ì§„ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. í…ì„œ ë‚´ì—ì„œ ìˆ«ìë¥¼ ì°¸ì¡°í•˜ëŠ” ë° í•„ìš”í•œ ì¸ë±ìŠ¤ì˜ ê°œìˆ˜ë¡œ ìˆœìœ„ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ì¹¼ë¼ëŠ” 0ìˆœìœ„ í…ì„œ(ì¸ë±ìŠ¤ ì—†ìŒ), ë²¡í„°ëŠ” 1ìˆœìœ„ í…ì„œ(ì¸ë±ìŠ¤ 1ê°œ, ì˜ˆ: v[i]), í–‰ë ¬ì€ 2ìˆœìœ„ í…ì„œ(ì¸ë±ìŠ¤ 2ê°œ, ì˜ˆ: a[i,j]), 3ìˆœìœ„ í…ì„œëŠ” ì§ìœ¡ë©´ì²´ ë˜ëŠ” "í–‰ë ¬ì˜ ìŠ¤íƒ"(ì¸ë±ìŠ¤ 3ê°œ, ì˜ˆ: b[i,j,k])ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, í…ì„œì˜ ë­í¬ëŠ” ëª¨ì–‘ì´ë‚˜ ì°¨ì›ê³¼ ë¬´ê´€í•˜ë©°, ì˜ˆë¥¼ ë“¤ì–´ 2x2x2 ëª¨ì–‘ì˜ í…ì„œì™€ 3x5x7 ëª¨ì–‘ì˜ í…ì„œëŠ” ëª¨ë‘ ë­í¬ 3ì„ ê°–ìŠµë‹ˆë‹¤.
"ë­í¬"ë¼ëŠ” ìš©ì–´ëŠ” í…ì„œ ë° í–‰ë ¬(ì„ í˜•ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ì—´ ë²¡í„°ì˜ ìˆ˜ë¥¼ ì˜ë¯¸)ì˜ ë§¥ë½ì—ì„œ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§ì— ìœ ì˜í•˜ì„¸ìš”.
  

6. What is the difference between tensor rank and shape? How do you get the rank from the shape?  
ë­í¬ëŠ” í…ì„œì˜ ì¶• ë˜ëŠ” ì°¨ì› ìˆ˜ì´ë©°, ëª¨ì–‘ì€ í…ì„œì˜ ê° ì¶•ì˜ í¬ê¸°ì…ë‹ˆë‹¤.
  

7. What are RMSE and L1 norm?  
L2 normì´ë¼ê³ ë„ í•˜ëŠ” í‰ê· ì œê³±ê·¼ì˜¤ì°¨(RMSE)ì™€ L1 normì´ë¼ê³ ë„ í•˜ëŠ” í‰ê· ì ˆëŒ€ì°¨(MAE)ëŠ” 'ê±°ë¦¬'ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì°¨ì´ëŠ” ì–´ë–¤ ì°¨ì´ëŠ” ì–‘ìˆ˜ì´ê³  ì–´ë–¤ ì°¨ì´ëŠ” ìŒìˆ˜ì—¬ì„œ ì„œë¡œ ìƒì‡„ë˜ê¸° ë•Œë¬¸ì— ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê±°ë¦¬ë¥¼ ì œëŒ€ë¡œ ì¸¡ì •í•˜ë ¤ë©´ ì°¨ì´ì˜ í¬ê¸°ì— ì´ˆì ì„ ë§ì¶˜ í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì°¨ì´ì˜ ì ˆëŒ€ê°’ì„ ë”í•˜ëŠ” ê²ƒì¸ë°, ì´ê²ƒì´ ë°”ë¡œ MAEì…ë‹ˆë‹¤. RMSEëŠ” ì œê³±ì˜ í‰ê· ì„ êµ¬í•œ ë‹¤ìŒ(ëª¨ë“  ê²ƒì„ ì–‘ìˆ˜ë¡œ í•¨) ì œê³±ê·¼ì„ êµ¬í•©ë‹ˆë‹¤(ì œê³±ì„ ë˜ëŒë¦¼).
  

8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?  
íŒŒì´ì¬ì—ì„œ ë£¨í”„ëŠ” ë§¤ìš° ëŠë¦¬ê¸° ë•Œë¬¸ì— ê°œë³„ ìš”ì†Œë¥¼ ë°˜ë³µí•˜ëŠ” ëŒ€ì‹  ì—°ì‚°ì„ ë°°ì—´ ì—°ì‚°ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•  ìˆ˜ ìˆë‹¤ë©´ ìˆœìˆ˜ Pythonë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ ê¸°ë³¸ C ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— NumPy ë˜ëŠ” PyTorchë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìˆ˜ì²œ ë°° ë” ë¹ ë¦…ë‹ˆë‹¤. ë” ì¢‹ì€ ì ì€ PyTorchë¥¼ ì‚¬ìš©í•˜ë©´ GPUì—ì„œ ì—°ì‚°ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³‘ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²½ìš° ì†ë„ê°€ í¬ê²Œ ë¹¨ë¼ì§‘ë‹ˆë‹¤.
  

9. Create a 3Ã—3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.  
```
    In [ ]: a = torch.Tensor(list(range(1,10))).view(3,3); print(a)
    Out [ ]: tensor([[1., 2., 3.],
                     [4., 5., 6.],
                     [7., 8., 9.]])
    In [ ]: b = 2 * a; print(b)
    Out [ ]: tensor([[ 2.,  4.,  6.],
                     [ 8., 10., 12.],
                     [14., 16., 18.]])
    In [ ]:  b[1:,1:]
    Out []: tensor([[10., 12.],
                    [16., 18.]])
```
  
10. What is broadcasting?  
NumPyë‚˜ PyTorchì™€ ê°™ì€ ê³¼í•™/ìˆ˜í•™ìš© íŒŒì´ì¬ íŒ¨í‚¤ì§€ëŠ” ì¢…ì¢… ì½”ë“œë¥¼ ë” ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆëŠ” ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ êµ¬í˜„í•©ë‹ˆë‹¤. PyTorchì˜ ê²½ìš°, ìˆœìœ„ê°€ ì‘ì€ í…ì„œëŠ” ìˆœìœ„ê°€ í° í…ì„œì™€ ë™ì¼í•œ í¬ê¸°ë¥¼ ê°–ë„ë¡ í™•ì¥ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ìˆœìœ„ê°€ ë‹¤ë¥¸ í…ì„œ ê°„ì— ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  

11. Are metrics generally calculated using the training set, or the validation set? Why?  
metricì€ ì¼ë°˜ì ìœ¼ë¡œ validation setì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤. validation setì€ ëª¨ë¸ì— ëŒ€í•´ ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì´ë¯€ë¡œ, ê³¼ì í•©ì´ ìˆëŠ”ì§€, ìœ ì‚¬í•œ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ validation setì—ì„œ ë©”íŠ¸ë¦­ì„ í‰ê°€í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŠµë‹ˆë‹¤.
  

12. What is SGD?  
SGD, ì¦‰ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ SGDëŠ” ì˜ˆì¸¡ ë° ëª©í‘œì— ëŒ€í•´ í‰ê°€ëœ ì£¼ì–´ì§„ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. SGD(ê·¸ë¦¬ê³  ë§ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜)ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ë¥¼ í†µí•´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì„ ì˜ ë°©ë²•ì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ SGDê°€ í•˜ëŠ” ì¼ì…ë‹ˆë‹¤.
  

13. Why does SGD use mini-batches?  
í•˜ë‚˜ ì´ìƒì˜ ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ ì†ì‹¤ í•¨ìˆ˜(ë° ê·¸ë˜ë””ì–¸íŠ¸)ë¥¼ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. ê³„ì‚° ì œí•œê³¼ ì‹œê°„ ì œì•½ìœ¼ë¡œ ì¸í•´ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë°˜ë³µí•˜ë©´ ê¸°ìš¸ê¸°ê°€ ë¶ˆì•ˆì •í•˜ê³  ë¶€ì •í™•í•´ì ¸ í›ˆë ¨ì— ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ íƒ€í˜‘í•˜ê¸° ìœ„í•´ ë°ì´í„° ì§‘í•©ì˜ ì‘ì€ í•˜ìœ„ ì§‘í•©ì— ëŒ€í•œ í‰ê·  ì†ì‹¤ì„ í•œ ë²ˆì— ê³„ì‚°í•©ë‹ˆë‹¤. ì´ í•˜ìœ„ ì§‘í•©ì„ ë¯¸ë‹ˆ ë°°ì¹˜ë¼ê³  í•©ë‹ˆë‹¤. ë˜í•œ ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ë©´ GPUì—ì„œ ë‹¨ì¼ í•­ëª©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì´ ë” ë†’ìŠµë‹ˆë‹¤.
  

14. What are the seven steps in SGD for machine learning?  
    (1) ë§¤ê°œë³€ìˆ˜ ì´ˆê¸°í™” - ì„ì˜ì˜ ê°’ì´ ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.  
    (2) ì˜ˆì¸¡ ê³„ì‚° - ì´ ì‘ì—…ì€ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë¯¸ë‹ˆ ë°°ì¹˜ì”© ìˆ˜í–‰ë©ë‹ˆë‹¤.  
    (3) ì†ì‹¤ ê³„ì‚° - ë¯¸ë‹ˆ ë°°ì¹˜ì— ëŒ€í•œ í‰ê·  ì†ì‹¤ì´ ê³„ì‚°ë©ë‹ˆë‹¤.  
    (4) ê¸°ìš¸ê¸° ê³„ì‚° - ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì–´ë–»ê²Œ ë³€ê²½í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ì…ë‹ˆë‹¤.  
    (5) ê°€ì¤‘ì¹˜ ë‹¨ê³„ - ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ì— ë”°ë¼ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  
    (6) ì´ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.  
    (7) ì¤‘ì§€ - ì‹¤ì œë¡œëŠ” ì‹œê°„ ì œì•½ì— ë”°ë¼ ë˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í›ˆë ¨/ê²€ì¦ ì†ì‹¤ ë° ì§€í‘œ ê°œì„ ì´ ì¤‘ë‹¨ë˜ëŠ” ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.  
  

15. How do we initialize the weights in a model?    
ëœë¤í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¤ë‹ˆë‹¤
  

16. What is "loss"?  
ì†ì‹¤ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì˜ˆì¸¡ê³¼ ëª©í‘œì— ë”°ë¼ ê°’ì„ ë°˜í™˜í•˜ë©°, ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë” ë‚˜ì€ ëª¨ë¸ ì˜ˆì¸¡ì— í•´ë‹¹í•©ë‹ˆë‹¤.
  

17. Why can't we always use a high learning rate?  
ì˜µí‹°ë§ˆì´ì €ê°€ ë„ˆë¬´ í° ì¡°ì¹˜ë¥¼ ì·¨í•˜ê³  ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜ˆìƒë³´ë‹¤ ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸í•˜ê¸° ë•Œë¬¸ì— ì†ì‹¤ì´ 'íŠ•ê¸°ê±°ë‚˜(ì§„ë™)' ì‹¬ì§€ì–´ ì°¨ì´ê°€ ë‚  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
  

18. What is a "gradient"?  
ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ëª¨ë¸ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ ë³€ê²½í•´ì•¼ í•˜ëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤. ì´ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(ë¯¸ë¶„)ë¥¼ ë³€ê²½í•  ë•Œ ì†ì‹¤ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„ì…ë‹ˆë‹¤.
  

19. Do you need to know how to calculate gradients yourself?  
ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìë™ìœ¼ë¡œ ê·¸ë¼ë°ì´ì…˜ì„ ê³„ì‚°í•˜ë¯€ë¡œ ê·¸ë¼ë°ì´ì…˜ì„ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì„ automatic differentiationì´ë¼ê³  í•©ë‹ˆë‹¤. PyTorchì—ì„œ requires_gradê°€ Trueì¸ ê²½ìš°, ì—­ë°©í–¥ ë©”ì„œë“œì¸ a.backward()ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  

20. Why can't we use accuracy as a loss function?  
ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•  ë•Œ ì†ì‹¤ í•¨ìˆ˜ê°€ ë³€ê²½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì •í™•ë„ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ë³€ê²½ë˜ëŠ” ê²½ìš°ì—ë§Œ ë³€ê²½ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì˜ˆì¸¡ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ í–¥ìƒë˜ì—ˆì§€ë§Œ ì˜ˆì¸¡ì´ ë³€ê²½ë˜ì§€ ì•ŠëŠ” ë“± ëª¨ë¸ì— ì•½ê°„ì˜ ë³€ê²½ì´ ìˆëŠ” ê²½ìš° ì •í™•ë„ëŠ” ì—¬ì „íˆ ë³€ê²½ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œ ì˜ˆì¸¡ì´ ë³€ê²½ë˜ëŠ” ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” ëª¨ë“  ê³³ì—ì„œ ê¸°ìš¸ê¸°ê°€ 0ì´ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ 0ê³¼ ê°™ì€ ê¸°ìš¸ê¸°ë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ì—†ìœ¼ë©°, ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šê³  í•™ìŠµë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¢‹ì€ ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì´ ì•½ê°„ ë” ë‚˜ì€ ì˜ˆì¸¡ì„ í•  ë•Œ ì•½ê°„ ë” ë‚˜ì€ ì†ì‹¤ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆì¸¡ì´ ì•½ê°„ ë” ì¢‹ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì— ëŒ€í•´ ë” í™•ì‹ ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, MNIST ì´ë¯¸ì§€ê°€ 3ì¼ í™•ë¥ ì„ 0.9 ëŒ€ 0.7ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•½ê°„ ë” ë‚˜ì€ ì˜ˆì¸¡ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” ì´ë¥¼ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
  

21. Draw the sigmoid function. What is special about its shape?  
![sigmoid function](https://forums.fast.ai/uploads/default/original/3X/1/1/11fa5da9a15e9b4db282ff9bc1f8237073173e7d.png)  
ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ëª¨ë“  ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ìª¼ê°œëŠ” ë¶€ë“œëŸ¬ìš´ ê³¡ì„ ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì´ 0ê³¼ 1 ì‚¬ì´ì˜ ì–´ë–¤ í˜•íƒœì˜ í™•ë¥  ë˜ëŠ” ì‹ ë¢° ìˆ˜ì¤€ì„ ì¶œë ¥í•œë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì´ë¥¼ ìœ„í•´ ëª¨ë¸ì˜ ë§ˆì§€ë§‰ì— ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
  

22. What is the difference between a loss function and a metric?  
ì¤‘ìš”í•œ ì°¨ì´ì ì€ ë©”íŠ¸ë¦­ì€ ì¸ê°„ì˜ ì´í•´ë¥¼ ìœ ë„í•˜ê³  ì†ì‹¤ì€ ìë™í™”ëœ í•™ìŠµì„ ìœ ë„í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì†ì‹¤ì´ í•™ìŠµì— ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë˜ë ¤ë©´ ì˜ë¯¸ ìˆëŠ” íŒŒìƒë¬¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì •í™•ë„ì™€ ê°™ì€ ë§ì€ ë©”íŠ¸ë¦­ì€ ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  ë©”íŠ¸ë¦­ì€ ì‚¬ëŒì´ ê´€ì‹¬ì„ ê°–ëŠ” ìˆ«ìë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë°˜ì˜í•©ë‹ˆë‹¤.
  

23. What is the function to calculate new weights using a learning rate?  
optimizerì˜ step function
  

24. What does the `DataLoader` class do?  
DataLoader í´ë˜ìŠ¤ëŠ” íŒŒì´ì¬ ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì™€ ì—¬ëŸ¬ ë°°ì¹˜ì— ê±¸ì¹œ ì´í„°ë ˆì´í„°ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  

25. Write pseudocode showing the basic steps taken in each epoch for SGD.  
```python
for x,y in dl:
     pred = model(x)
     loss = loss_func(pred, y)
     loss.backward()
     parameters -= parameters.grad * lr
```
  

26. Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?  
```def func(a,b): return list(zip(a,b))```  
ì´ ë°ì´í„° êµ¬ì¡°ëŠ” ê° íŠœí”Œì— ì…ë ¥ ë°ì´í„°ì™€ ë ˆì´ë¸”ì´ í¬í•¨ëœ íŠœí”Œ ëª©ë¡ì´ í•„ìš”í•  ë•Œ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì— ìœ ìš©í•©ë‹ˆë‹¤.
  

27. What does `view` do in PyTorch?  
í…ì„œì˜ ë‚´ìš©ì„ ë³€ê²½í•˜ì§€ ì•Šê³  í…ì„œì˜ ëª¨ì–‘ì„ ë³€ê²½í•©ë‹ˆë‹¤.
  

28. What are the "bias" parameters in a neural network? Why do we need them?    
bias íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ì…ë ¥ì´ 0ì¼ ë•Œ, ì¶œë ¥ì€ í•­ìƒ 0ì´ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë°”ì´ì–´ìŠ¤ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì— ì¶”ê°€ì ì¸ ìœ ì—°ì„±ì´ ì¶”ê°€ë©ë‹ˆë‹¤.
  

29. What does the `@` operator do in Python?    
Decoratorì…ë‹ˆë‹¤. ë©”ì¸ êµ¬ë¬¸ì´ ìˆê³ , ì—¬ê¸°ì— ë¶€ê°€ì ì¸ êµ¬ë¬¸ì„ ì¶”ê°€í•˜ê³  ì‹¶ì„ ë•Œ. ê·¸ë¦¬ê³  ë¶€ê°€ì ì¸ êµ¬ë¬¸ì„ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ ë¶€ê°€ì ì¸(ê·¸ë¦¬ê³  ë°˜ë³µì ì¸) ì‘ì—…ì„ Decoratorë¡œ ì„ ì–¸í•´ì„œ ììœ ë¡­ê²Œ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  

30. What does the `backward` method do?    
ì´ ë©”ì„œë“œëŠ” í˜„ì¬ gradientë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
  

31. Why do we have to zero the gradients?  
PyTorchëŠ” ë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì´ì „ì— ì €ì¥ëœ ê¸°ìš¸ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤. ê¸°ìš¸ê¸°ë¥¼ ì œë¡œí™”í•˜ì§€ ì•Šê³  í›ˆë ¨ ë£¨í”„ í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ë©´ ì´ì „ì— ì €ì¥ëœ ê¸°ìš¸ê¸° ê°’ì— í˜„ì¬ ì†ì‹¤ì˜ ê¸°ìš¸ê¸°ê°€ ì¶”ê°€ë©ë‹ˆë‹¤.
  

32. What information do we have to pass to `Learner`?  
DataLoader, ëª¨ë¸, ìµœì í™” í•¨ìˆ˜, ì†ì‹¤ í•¨ìˆ˜, ê·¸ë¦¬ê³  ì„ íƒì ìœ¼ë¡œ ì¸ì‡„í•  metricì„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
  
   
33. Show Python or pseudocode for the basic steps of a training loop.    
```python
def train_epoch(model, lr, params):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
for i in range(20):
    train_epoch(model, lr, params)
```
  

34. What is "ReLU"? Draw a plot of it for values from `-2` to `+2`.    
ReLUëŠ” "ìŒìˆ˜ë¥¼ 0ìœ¼ë¡œ ë°”ê¾¸ê¸°"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì…ë‹ˆë‹¤.    
![ReLU function](https://forums.fast.ai/uploads/default/original/3X/2/c/2ccd17a969b3e3547b34c5823066d5874415db71.png)
  

35. What is an "activation function"?    
í™œì„±í™” í•¨ìˆ˜ëŠ” ì‹ ê²½ë§ì˜ ì¼ë¶€ì¸ ë˜ ë‹¤ë¥¸ í•¨ìˆ˜ë¡œ, ëª¨ë¸ì— ë¹„ì„ í˜•ì„±ì„ ì œê³µí•˜ëŠ” ë° ëª©ì ì´ ìˆìŠµë‹ˆë‹¤. í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ y=mx+b í˜•ì‹ì˜ ì„ í˜• í•¨ìˆ˜ê°€ ì—¬ëŸ¬ ê°œ ìˆì„ ë¿ì´ë¼ëŠ” ê°œë…ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ë ¨ì˜ ì„ í˜• ë ˆì´ì–´ëŠ” í•˜ë‚˜ì˜ ì„ í˜• ë ˆì´ì–´ì™€ ë™ì¼í•˜ë¯€ë¡œ ëª¨ë¸ì€ ë°ì´í„°ì— ì„  í•˜ë‚˜ë§Œ ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„ í˜• ë ˆì´ì–´ ì‚¬ì´ì— ë¹„ì„ í˜•ì„±ì„ ë„ì…í•˜ë©´ ë” ì´ìƒ ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ë ˆì´ì–´ëŠ” ë‚˜ë¨¸ì§€ ë ˆì´ì–´ì™€ ì–´ëŠ ì •ë„ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë©°, ì´ì œ ëª¨ë¸ì€ í›¨ì”¬ ë” ë³µì¡í•œ í•¨ìˆ˜ë¥¼ ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ ì´ëŸ¬í•œ ëª¨ë¸ì€ ì˜¬ë°”ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶©ë¶„íˆ í° ëª¨ë¸ì´ë¼ë©´ ê³„ì‚° ê°€ëŠ¥í•œ ëª¨ë“  ë¬¸ì œë¥¼ ì„ì˜ë¡œ ë†’ì€ ì •í™•ë„ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ universal approximation theoremì´ë¼ê³  í•©ë‹ˆë‹¤.
  

36. What's the difference between `F.relu` and `nn.ReLU`?    
F.reluëŠ” ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ìœ„í•œ íŒŒì´ì¬ í•¨ìˆ˜ì…ë‹ˆë‹¤. ë°˜ë©´ì— nn.ReLUëŠ” PyTorch ëª¨ë“ˆì…ë‹ˆë‹¤.
  

37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?    
ë‘˜ ì´ìƒì˜ ë¹„ì„ í˜•ì„±ì„ ì‚¬ìš©í•˜ë©´ ì‹¤ì§ˆì ì¸ ì„±ëŠ¥ ì´ì ì´ ìˆìŠµë‹ˆë‹¤. ë” ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ë” ì‹¬ì¸µì ì¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , ì„±ëŠ¥ì´ í–¥ìƒë˜ë©°, í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , ì»´í“¨íŒ…/ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.
  

### Further Research

1. Create your own implementation of `Learner` from scratch, based on the training loop shown in this chapter.
2. Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way.
